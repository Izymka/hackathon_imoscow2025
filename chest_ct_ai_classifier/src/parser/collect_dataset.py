import argparse
import csv
from datetime import datetime
import io
import logging
import os
import shutil
import tarfile
import time
from pathlib import Path

import requests
from dotenv import load_dotenv

from chest_ct_ai_classifier.src.utils.dicom_parser import parse_dicom

logging.basicConfig(level=logging.INFO)
load_dotenv()

USE_ROBOCOPY = os.name == 'nt'
if USE_ROBOCOPY:
    import subprocess
    check_network = subprocess.run(['net', 'use'], capture_output=True, text=True)


def extract_archive(file, dicom_path):
    # If this is a file and looks like an archive, check for extracted directory
    try:
        if file.is_file():
            name_lower = file.name.lower()
            # handle tar.gz and tgz
            if name_lower.endswith(".tar.gz") or name_lower.endswith(".tgz"):
                # determine base name without the compression extensions
                if name_lower.endswith(".tar.gz"):
                    base_name = file.name[:-7]  # strip ".tar.gz"
                else:
                    base_name = file.name[:-4]  # strip ".tgz"
                target_dir = dicom_path / base_name

                if not target_dir.exists() or not target_dir.is_dir():
                    logging.info("Archive found: %s. Extracting to: %s", file.name, target_dir)
                    # ensure target directory exists
                    target_dir.mkdir(parents=True, exist_ok=True)
                    try:
                        # open with auto-detection for compression
                        with tarfile.open(file, "r:*") as tf:
                            tf.extractall(path=target_dir)
                        logging.info("Extraction completed: %s", target_dir)
                    except Exception:
                        logging.exception("Failed to extract tar archive: %s", file)
                else:
                    logging.info("Already extracted: %s", target_dir)

            # optionally handle zip archives as well
            elif name_lower.endswith(".zip"):
                base_name = file.name[:-4]  # strip ".zip"
                target_dir = dicom_path / base_name

                if not target_dir.exists() or not target_dir.is_dir():
                    logging.info("ZIP archive found: %s. Extracting to: %s", file.name, target_dir)
                    target_dir.mkdir(parents=True, exist_ok=True)
                    try:
                        shutil.unpack_archive(str(file), str(target_dir))
                        logging.info("ZIP extraction completed: %s", target_dir)
                    except Exception:
                        logging.exception("Failed to extract zip archive: %s", file)
                else:
                    logging.info("Already extracted: %s", target_dir)
    except PermissionError:
        logging.error("Permission denied when processing file: %s", file)
    except Exception:
        logging.exception("Error processing file: %s", file)


def write_to_csv(result, output_file, clear_file=False):
    fieldnames = ['filename', 'label']

    if not Path(output_file).exists():
        # –°–æ–∑–¥–∞–µ–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerow(result)
    else:
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Ñ–∞–π–ª—É
        with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
            writer.writerow(result)

def is_google_spreadsheet_url(url_or_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø—É—Ç—å —Å—Å—ã–ª–∫–æ–π –Ω–∞ Google Spreadsheet"""
    if isinstance(url_or_path, str):
        return url_or_path.startswith('https://docs.google.com/spreadsheets/')
    return False


def convert_google_sheet_url_to_csv(google_sheet_url):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ Google Spreadsheet –≤ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è CSV"""
    if '/edit' in google_sheet_url:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        sheet_id = google_sheet_url.split('/d/')[1].split('/')[0]
        return f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv'
    return google_sheet_url


def download_csv_from_google_sheet(google_sheet_url, encoding='utf-8'):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç CSV –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Spreadsheet –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫"""
    try:
        csv_url = convert_google_sheet_url_to_csv(google_sheet_url)
        logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Spreadsheet: %s", csv_url)

        response = requests.get(csv_url, timeout=30)
        response.raise_for_status()

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        csv_content = response.content.decode(encoding)

        # –ß–∏—Ç–∞–µ–º CSV –∏–∑ —Å—Ç—Ä–æ–∫–∏
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        rows = list(csv_reader)

        logging.info("–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ %d —Å—Ç—Ä–æ–∫ –∏–∑ Google Spreadsheet", len(rows))
        return rows

    except requests.exceptions.RequestException as e:
        logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Google Spreadsheet: %s", e)
        raise
    except Exception as e:
        logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ CSV –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Spreadsheet: %s", e)
        raise



def run():
    parser = argparse.ArgumentParser(description='Move CT dicom files to target dataset')
    parser.add_argument('--source', help='Source directory')
    parser.add_argument('--target', help='Target directory')
    parser.add_argument('--studies_csv', help='CSV file with studies list')
    parser.add_argument('--recollect', help='Recollect')
    parser.add_argument('--csv_delim', default=',', help='csv delimeter')
    parser.add_argument('--csv_encoding', default='utf-8', help='csv encoding')
    parser.add_argument('--read-study-id-from-dicom', default=False)
    parser.add_argument('--transfer', default=True)
    parser.add_argument("--multiple-series-strategy",
                        default="skip",
                        choices=["skip", "first", "rich", "largest"],
                        help="How to handle multiple series in a study")

    args = parser.parse_args()

    config_logger()

    reading_study_id_from_dicom = args.read_study_id_from_dicom
    do_transfer = str(args.transfer).lower() in ('true', '1', 't', 'y', 'yes')

    source_path = Path(args.source)
    if not source_path.exists():
        logging.error("Source path does not exist: %s", source_path)
        exit(1)
    target_path = Path(args.target)
    if not target_path.exists():
        if target_path.parent.exists():
            # create target directory
            target_path.mkdir(parents=True, exist_ok=True)
            logging.info("Created target directory: %s", target_path)
        else:
            logging.error("Target path and parent does not exist: %s", target_path)
            exit(1)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Ä–µ–µ—Å—Ç—Ä–∞
    if is_google_spreadsheet_url(args.studies_csv):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Spreadsheet
        try:
            rows = download_csv_from_google_sheet(args.studies_csv, args.csv_encoding)
        except Exception as e:
            logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Spreadsheet: %s", e)
            exit(1)
    else:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
        studies_csv_file = Path(args.studies_csv)
        if not studies_csv_file.exists():
            logging.error("Studies CSV file does not exist: %s", studies_csv_file)
            exit(1)

        # –ß–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Ä–µ–µ—Å—Ç—Ä–∞
        with open(studies_csv_file, 'r', encoding=args.csv_encoding) as csvfile:
            csvfile = csv.DictReader(csvfile, delimiter=args.csv_delim)
            rows = list(csvfile)

    def move_dicom_files(dicom_series_path, target_dir):
        dicom_series_path = Path(os.path.normpath(str(dicom_series_path)))
        start_time = time.time()
        dirs = [f for f in dicom_series_path.iterdir() if f.is_dir()]
        if dirs:
            if len(dirs) > 1 or args.multiple_series_strategy == "first":
                if args.multiple_series_strategy == "skip":
                    logging.warning("  üôÖ  Found %d series folders in: %s. Skipping", len(dirs), dicom_series_path)
                    return False
                if args.multiple_series_strategy == "largest":
                    # Find directory with most files
                    largest_dir = max(dirs, key=lambda d: len([f for f in d.iterdir() if not f.name.startswith('.')]))
                    logging.info("  üìÅ  Selected largest series folderfrom %d folders: %s",len(dirs), largest_dir.name)
                    return move_dicom_files(largest_dir, target_dir)
            return move_dicom_files(dirs[0], target_dir)
        files = [f for f in dicom_series_path.iterdir() if not f.name.startswith('.')]
        if not files:
            logging.error("  üö´  No files found in: %s", dicom_series_path)
            return False
        if USE_ROBOCOPY:
            subprocess.run(['robocopy', str(dicom_series_path), str(target_dir)], capture_output=True)
        else:
            target_dir.mkdir(parents=True, exist_ok=True)
            for file in files:
                shutil.copy(str(file), str(target_dir))
        elapsed_time = time.time() - start_time
        logging.info("  üöÄ  Moved %d files from %s to %s in %.2f seconds", len(files), dicom_series_path, target_dir,
                     elapsed_time)
        return True

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    start_time = time.time()
    stats = {
        'total': 0,
        'processed': 0,
        'skipped_no_dicom': 0,
        'skipped_no_study_id': 0,
        'skipped_multiple_series': 0,
        'already_exists': 0,
        'errors': 0
    }

    i = 0
    for row in rows:
        i += 1
        study_id = row.get('id', '').strip()
        stats['total'] += 1
        logging.info("[%d/%d] Processing study ID: %s", i, len(rows), study_id)

        try:
            dicom_dir = source_path / study_id
            if not dicom_dir.exists():
                dcom_tar = source_path / (study_id + '.tar.gz')
                if not dcom_tar.exists():
                    logging.warning("  ‚ùå  DICOM directory and archive not found for study_id: %s", dicom_dir)
                    stats['skipped_no_dicom'] += 1
                    continue
                extract_archive(dcom_tar, source_path)

            if reading_study_id_from_dicom:
                summary = parse_dicom(dicom_dir)
                study_id = summary.study_uid
                if not study_id:
                    logging.error('  ‚ùå  Unable to read study ID from DICOM directory: %s', dicom_dir)
                    stats['skipped_no_study_id'] += 1
                    continue

            if dicom_dir.exists():
                logging.info("  üïµÔ∏è  Found DICOM directory: %s", dicom_dir)
                if study_id.endswith('.nii'):
                    study_id = study_id[:-4]
                target_dir = target_path / study_id
                if do_transfer:
                    if not target_dir.exists() or len(list(target_dir.iterdir())) == 0:
                        logging.info("  üéØ  Target directory: %s", target_dir)
                        move_result = move_dicom_files(dicom_dir, target_dir)
                        if move_result is False:
                            # move_dicom_files returned False (multiple series skip)
                            stats['skipped_multiple_series'] += 1
                        elif move_result:
                            write_to_csv({
                                'filename': study_id + '.pt',
                                'label': row.get('patology')
                            }, target_path / 'labels.csv')
                            stats['processed'] += 1
                    else:
                        logging.info("  üòè  Target directory already exists: %s", target_dir)
                        stats['already_exists'] += 1
                else:
                    logging.info("  ‚úÖ OK [transfer disabled]")
                    stats['processed'] += 1
            else:
                logging.warning("  ü§î  DICOM directory not found: %s", dicom_dir)
                stats['skipped_no_dicom'] += 1
        except Exception as e:
            logging.error("  üí•  Error processing study %s: %s", study_id, str(e))
            stats['errors'] += 1

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    end_time = time.time()
    total_time = end_time - start_time

    logging.info("=" * 80)
    logging.info("üéØ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø")
    logging.info("=" * 80)
    logging.info("üìä –í—Å–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: %d", stats['total'])
    logging.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: %d", stats['processed'])
    logging.info("üè† –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: %d", stats['already_exists'])
    logging.info("‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç DICOM): %d", stats['skipped_no_dicom'])
    logging.info("üÜî –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç Study ID): %d", stats['skipped_no_study_id'])
    logging.info("üìÅ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–µ—Ä–∏–∏): %d", stats['skipped_multiple_series'])
    logging.info("üí• –û—à–∏–±–∫–∏: %d", stats['errors'])
    logging.info("‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: %.2f —Å–µ–∫ (%.2f –º–∏–Ω)", total_time, total_time / 60)

    if stats['total'] > 0:
        success_rate = (stats['processed'] + stats['already_exists']) / stats['total'] * 100
        logging.info("üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏: %.1f%%", success_rate)

    logging.info("=" * 80)


def config_logger():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é logs, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)

    log_filename = logs_dir / f"collect_dataset_{timestamp}.log"

    # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –ª–æ–≥–æ–≤
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ª–æ–≥–≥–µ—Ä
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ñ–∞–π–ª–∞
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    logging.info("–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞. –õ–æ–≥ —Ñ–∞–π–ª: %s", log_filename)


if __name__ == '__main__':
    run()
