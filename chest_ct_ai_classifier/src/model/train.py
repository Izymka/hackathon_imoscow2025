import os
import time
import logging
import torch
import numpy as np
from torch import nn, optim
from torch.utils.data import DataLoader
from setting import parse_opts
from datasets.medical_tensors import MedicalTensorDataset
from model_generator import generate_model


def setup_logging(log_file='logs/train.log'):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–≥–µ—Ä–∞."""
    logger = logging.getLogger('train_logger')
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s',
                                  datefmt='%Y-%m-%d %H:%M:%S')

    # –ö–æ–Ω—Å–æ–ª—å
    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    # –§–∞–π–ª
    fh = logging.FileHandler(log_file)
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    return logger

def build_optimizer(model, cfg):
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å —Ä–∞–∑–Ω—ã–º–∏ learning rate
    –¥–ª—è –Ω–æ–≤—ã—Ö –∏ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–µ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É.
    """
    if not cfg.use_differential_lr:
        # –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å
        return torch.optim.Adam(
            model.parameters(),
            lr=cfg.learning_rate,
            weight_decay=cfg.weight_decay
        )

    # ---------- –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ----------
    # –Ω–æ–≤—ã–µ —Å–ª–æ–∏ (—Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –¥–ª—è fine-tuning)
    new_params = []
    base_params = []
    for name, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if any(layer in name for layer in cfg.new_layer_names):
            new_params.append(p)
        else:
            base_params.append(p)

    optimizer = torch.optim.Adam(
        [
            {"params": base_params,
             "lr": cfg.learning_rate * cfg.base_lr_multiplier},
            {"params": new_params,
             "lr": cfg.learning_rate * cfg.new_layers_lr_multiplier}
        ],
        weight_decay=cfg.weight_decay
    )
    return optimizer


def train(data_loader, model, optimizer, scheduler,
          total_epochs, save_interval, save_folder, sets):

    logger = setup_logging()
    device = torch.device("cuda" if (torch.cuda.is_available() and not sets.no_cuda) else "cpu")
    print(f"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ [x]: {device}")

    # === FIX: –¥–æ–±–∞–≤–ª–µ–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π –ø–æ—Ç–µ—Ä—å ===
    loss_cls = nn.CrossEntropyLoss().to(device)

    model.to(device)
    model.train()

    # === FIX: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ ===
    batches_per_epoch = len(data_loader)
    train_time_sp = time.time()

    for epoch in range(total_epochs):
        logger.info('Start epoch %d', epoch + 1)

        # ExponentialLR ‚Äì step() –≤ –Ω–∞—á–∞–ª–µ —ç–ø–æ—Ö–∏
        if isinstance(scheduler, optim.lr_scheduler.ExponentialLR):
            scheduler.step()

        # –¢–µ–∫—É—â–∏–π lr
        current_lr = scheduler.get_last_lr()[0]
        logger.info('Learning rate: %.6f', current_lr)

        for batch_id, (volumes, labels) in enumerate(data_loader):
            batch_id_sp = epoch * batches_per_epoch + batch_id

            volumes, labels = volumes.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(volumes)
            loss = loss_cls(outputs, labels)
            loss.backward()
            optimizer.step()

            # Accuracy
            _, predicted = torch.max(outputs, 1)
            correct = (predicted == labels).sum().item()
            accuracy = 100.0 * correct / labels.size(0)

            avg_batch_time = (time.time() - train_time_sp) / (1 + batch_id_sp)
            logger.info(
                'Epoch[%d/%d] Batch[%d/%d] loss=%.4f acc=%.2f%% avg_time=%.3fs',
                epoch + 1, total_epochs, batch_id + 1, batches_per_epoch,
                loss.item(), accuracy, avg_batch_time
            )

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            if (not sets.ci_test and batch_id == 0
                and batch_id_sp != 0 and batch_id_sp % save_interval == 0):
                model_save_path = os.path.join(
                    save_folder, f'epoch_{epoch+1}_batch_{batch_id+1}.pth.tar'
                )
                os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
                torch.save({
                    'epoch': epoch + 1,
                    'batch_id': batch_id + 1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict()
                }, model_save_path)
                logger.info('Checkpoint saved: %s', model_save_path)

        # ReduceLROnPlateau ‚Äì step() –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏
        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(loss.item())

    logger.info('Training finished')


if __name__ == '__main__':
    os.makedirs('logs', exist_ok=True)
    sets = parse_opts()

    if sets.ci_test:
        sets.img_list = '../toy_data/test_ci.txt'
        sets.n_epochs = 1
        sets.no_cuda = True
        sets.data_root = '../toy_data'
        sets.pretrain_path = ''
        sets.num_workers = 0
        sets.model_depth = 10
        sets.resnet_shortcut = 'A'
        sets.input_D, sets.input_H, sets.input_W = 14, 28, 28

    torch.manual_seed(sets.manual_seed)
    model, parameters = generate_model(sets)

    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ ===
    # generate_model —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å:
    #   1) –æ–±—ã—á–Ω—ã–π iterable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Å—Ç–∞—Ä—ã–π —Å–ª—É—á–∞–π)
    #   2) dict {'base_parameters': ..., 'new_parameters': ...}
    #      –µ—Å–ª–∏ –≤—ã –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–º–µ—Ç–∏–ª–∏ –Ω–æ–≤—ã–µ —Å–ª–æ–∏ (fc/layer3/layer4)
    if isinstance(parameters, dict):
        # --- DIFF-LR ---
        # –±–∞–∑–æ–≤—ã–µ —Å–ª–æ–∏ —É—á–∏–º –º–µ–¥–ª–µ–Ω–Ω–æ
        base_lr = sets.learning_rate * getattr(sets, 'base_lr_multiplier', 0.1)
        # –Ω–æ–≤—ã–µ —Å–ª–æ–∏ (fc, layer3/4) —É—á–∏–º –±—ã—Å—Ç—Ä–µ–µ
        new_lr = sets.learning_rate * getattr(sets, 'new_layers_lr_multiplier', 1.0)
        params = [
            {'params': parameters['base_parameters'], 'lr': sets.learning_rate},
            {'params': parameters['new_parameters'], 'lr': sets.learning_rate * 100}
        ]
        print(f"üí° Differential LR: base={base_lr:.2e}, new={new_lr:.2e}")
    else:
        # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–ª–∏
        params = [{'params': list(parameters), 'lr': sets.learning_rate}]
        print(f"üí° Single LR: {sets.learning_rate:.2e}")

    # –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = optim.SGD(
        params,
        momentum=0.9,
        weight_decay=getattr(sets, 'weight_decay', 1e-3)
    )
    # –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ (–ø—Ä–∏–º–µ—Ä ‚Äì —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π)
    scheduler = optim.lr_scheduler.ExponentialLR(
        optimizer,
        gamma=getattr(sets, 'lr_gamma', 0.99)
    )

    # === –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å) ===
    if sets.resume_path and os.path.isfile(sets.resume_path):
        checkpoint = torch.load(
            sets.resume_path,
            map_location='cpu' if sets.no_cuda else None
        )
        state_dict = checkpoint['state_dict']
        from collections import OrderedDict
        new_state_dict = OrderedDict(
            (k[7:] if k.startswith('module.') else k, v)
            for k, v in state_dict.items()
        )
        model.load_state_dict(new_state_dict, strict=False)
        optimizer.load_state_dict(checkpoint.get('optimizer', optimizer.state_dict()))

    # === –î–∞–Ω–Ω—ã–µ ===
    sets.phase = 'train'
    sets.pin_memory = not sets.no_cuda
    dataset = MedicalTensorDataset(sets.data_root, sets.img_list, sets)
    loader = DataLoader(dataset, batch_size=sets.batch_size,
                        shuffle=True, num_workers=sets.num_workers,
                        pin_memory=sets.pin_memory)

    # === –û–±—É—á–µ–Ω–∏–µ ===
    train(loader, model, optimizer, scheduler,
          total_epochs=sets.n_epochs,
          save_interval=sets.save_intervals,
          save_folder=sets.save_folder,
          sets=sets)
