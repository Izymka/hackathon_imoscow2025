
"""
inference_3d_4chanel.py

–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ 3D ResNet –º–æ–¥–µ–ª–∏ –Ω–∞ 4-–∫–∞–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–∞—Ö 128x128x128.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Å–∞ –∏–∑ checkpoint —Ñ–∞–π–ª–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã.
"""

import os
import argparse
import logging
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
from tqdm import tqdm
import sys


sys.path.append('model/outputs/weights')  # –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
from models import resnet


def setup_logging(log_file: Path):
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


class InferenceDataset(Dataset):
    """Dataset –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ .pt —Ñ–∞–π–ª—ã"""

    def __init__(self, tensor_paths):
        self.tensor_paths = tensor_paths

    def __len__(self):
        return len(self.tensor_paths)

    def __getitem__(self, idx):
        tensor_path = self.tensor_paths[idx]
        tensor = torch.load(tensor_path, weights_only=True)

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É [1, 4, 128, 128, 128]
        if tensor.shape != (1, 4, 128, 128, 128):
            raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {tensor.shape}, –æ–∂–∏–¥–∞–µ—Ç—Å—è (1, 4, 128, 128, 128)")

        return tensor, str(tensor_path)


def adapt_first_conv_layer_to_4ch(model, pretrained_state_dict=None):
    """
    –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø–µ—Ä–≤—ã–π —Å–≤—ë—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–¥ 4-–∫–∞–Ω–∞–ª—å–Ω—ã–π –≤—Ö–æ–¥.
    """
    device = next(model.parameters()).device

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
    if pretrained_state_dict is not None:
        # –£–±–∏—Ä–∞–µ–º 'module.' –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        cleaned_state_dict = {}
        for k, v in pretrained_state_dict.items():
            new_k = k.replace('module.', '') if k.startswith('module.') else k
            cleaned_state_dict[new_k] = v
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å strict=False, —Ç–∞–∫ –∫–∞–∫ conv1 –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
        model.load_state_dict(cleaned_state_dict, strict=False)

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ—Å conv1 (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å [C_out, 1, K, K, K])
    original_weight = model.conv1.weight.data  # [64, 1, 7, 7, 7] –¥–ª—è ResNet-18/34
    assert original_weight.shape[1] == 1, f"–û–∂–∏–¥–∞–ª—Å—è 1 –≤—Ö–æ–¥–Ω–æ–π –∫–∞–Ω–∞–ª, –ø–æ–ª—É—á–µ–Ω–æ {original_weight.shape[1]}"

    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –≤–µ—Å: –ø–æ–≤—Ç–æ—Ä—è–µ–º –∏ –¥–µ–ª–∏–º –Ω–∞ 4 –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞
    new_weight = original_weight.repeat(1, 4, 1, 1, 1) / 4.0  # [64, 4, 7, 7, 7]

    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π conv1 —Å–ª–æ–π
    new_conv1 = torch.nn.Conv3d(
        in_channels=4,
        out_channels=original_weight.shape[0],
        kernel_size=model.conv1.kernel_size,
        stride=model.conv1.stride,
        padding=model.conv1.padding,
        bias=model.conv1.bias is not None
    )
    new_conv1.weight.data = new_weight
    if new_conv1.bias is not None:
        new_conv1.bias.data = model.conv1.bias.data.clone()

    # –ó–∞–º–µ–Ω—è–µ–º –≤ –º–æ–¥–µ–ª–∏
    model.conv1 = new_conv1.to(device)
    print(f"‚úÖ –ó–∞–º–µ–Ω—ë–Ω conv1: {original_weight.shape} ‚Üí {new_weight.shape}")
    return model


def create_model(model_depth, n_classes, checkpoint_path=None):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–∞ –∏–∑ checkpoint"""

    # –ú–∞–ø–ø–∏–Ω–≥ –≥–ª—É–±–∏–Ω –º–æ–¥–µ–ª–∏
    model_functions = {
        10: resnet.resnet10,
        18: resnet.resnet18,
        34: resnet.resnet34,
        50: resnet.resnet50,
        101: resnet.resnet101,
        152: resnet.resnet152,
        200: resnet.resnet200
    }

    if model_depth not in model_functions:
        raise ValueError(f"Model depth {model_depth} not supported. Available: {list(model_functions.keys())}")

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å 1 –∫–∞–Ω–∞–ª–æ–º (–∫–∞–∫ –≤ MedicalNet)
    model = model_functions[model_depth](
        sample_input_W=128,
        sample_input_H=128,
        sample_input_D=128,
        shortcut_type='A',  # –∏–ª–∏ 'B' –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        no_cuda=False,
        num_seg_classes=2
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
    if checkpoint_path:
        print(f'üì• –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint: {checkpoint_path}')
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É checkpoint
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤–µ—Å–∞ –Ω–∞–ø—Ä—è–º—É—é

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        model.load_state_dict(state_dict, strict=False)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(state_dict)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ checkpoint")

    # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –ø–æ–¥ 4 –∫–∞–Ω–∞–ª–∞
    model = adapt_first_conv_layer_to_4ch(model)

    return model


def run_inference(model, dataloader, device, output_dir, save_probabilities=True, save_logits=True):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–∏"""

    model.eval()
    results = []

    print(f"üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ {len(dataloader.dataset)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")

    with torch.no_grad():
        for batch_idx, (batch_tensors, tensor_paths) in enumerate(tqdm(dataloader, desc="–ò–Ω—Ñ–µ—Ä–µ–Ω—Å")):
            batch_tensors = batch_tensors.to(device)

            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(batch_tensors)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            probabilities = F.softmax(outputs, dim=1)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –≤ –±–∞—Ç—á–µ
            for i in range(len(tensor_paths)):
                tensor_path = Path(tensor_paths[i])
                patient_name = tensor_path.stem

                result = {
                    'patient_name': patient_name,
                    'tensor_path': str(tensor_path),
                    'logits': outputs[i].cpu().numpy(),
                    'probabilities': probabilities[i].cpu().numpy()
                }

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                if save_logits:
                    logits_path = output_dir / f"{patient_name}_logits.pt"
                    torch.save(outputs[i].cpu(), logits_path)
                    result['logits_path'] = str(logits_path)

                if save_probabilities:
                    probs_path = output_dir / f"{patient_name}_probs.pt"
                    torch.save(probabilities[i].cpu(), probs_path)
                    result['probs_path'] = str(probs_path)

                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
                predicted_class = torch.argmax(probabilities[i]).item()
                result['predicted_class'] = predicted_class
                result['confidence'] = probabilities[i][predicted_class].item()

                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                result['all_probabilities'] = probabilities[i].cpu().numpy().tolist()

                results.append(result)

    return results


def main():
    parser = argparse.ArgumentParser(description="–ò–Ω—Ñ–µ—Ä–µ–Ω—Å 3D ResNet –º–æ–¥–µ–ª–∏")
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="–ü—É—Ç—å –∫ checkpoint —Ñ–∞–π–ª—É (.pth, .pt)")
    parser.add_argument("--input", type=str, required=True,
                        help="–í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å .pt —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ –∏–ª–∏ –ø—É—Ç—å –∫ –æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É")
    parser.add_argument("--output", type=str, required=True,
                        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--model-depth", type=int, default=18,
                        choices=[10, 18, 34, 50, 101, 152, 200],
                        help="–ì–ª—É–±–∏–Ω–∞ ResNet –º–æ–¥–µ–ª–∏")
    parser.add_argument("--num-classes", type=int, required=True,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    parser.add_argument("--batch-size", type=int, default=1,
                        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–æ–±—ã—á–Ω–æ 1 –¥–ª—è 3D –¥–∞–Ω–Ω—ã—Ö)")
    parser.add_argument("--device", type=str, default="cuda",
                        choices=["cuda", "cpu"],
                        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    parser.add_argument("--no-probabilities", action="store_true",
                        help="–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏")
    parser.add_argument("--no-logits", action="store_true",
                        help="–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–æ–≥–∏—Ç—ã")
    parser.add_argument("--verbose", action="store_true",
                        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--log-file", type=str, default="logs/inference.log",
                        help="–§–∞–π–ª –ª–æ–≥–∞")

    args = parser.parse_args()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging(Path(args.log_file))

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if args.device == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU")
    else:
        device = torch.device("cpu")
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # –ü–æ–∏—Å–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤
    input_path = Path(args.input)
    if input_path.is_file() and input_path.suffix == '.pt':
        tensor_paths = [input_path]
    elif input_path.is_dir():
        tensor_paths = list(input_path.glob("*.pt"))
        if not tensor_paths:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ .pt —Ñ–∞–π–ª–æ–≤ –≤ {input_path}")
    else:
        raise ValueError(f"–í—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è .pt —Ñ–∞–π–ª–æ–º –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {input_path}")

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(tensor_paths)} —Ç–µ–Ω–∑–æ—Ä–æ–≤ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–∞
    dataset = InferenceDataset(tensor_paths)
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ResNet-{args.model_depth} —Å {args.num_classes} –∫–ª–∞—Å—Å–∞–º–∏")
    model = create_model(args.model_depth, args.num_classes, args.checkpoint)
    model = model.to(device)

    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}, –æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}")

    # –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    results = run_inference(
        model=model,
        dataloader=dataloader,
        device=device,
        output_dir=output_dir,
        save_probabilities=not args.no_probabilities,
        save_logits=not args.no_logits
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞
    report = {
        'config': {
            'checkpoint': args.checkpoint,
            'input_dir': str(input_path),
            'output_dir': str(output_dir),
            'model_depth': args.model_depth,
            'num_classes': args.num_classes,
            'batch_size': args.batch_size,
            'device': str(device),
            'total_samples': len(results)
        },
        'results': results
    }

    report_path = output_dir / "inference_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_counts = {}
    total_confidence = 0
    for result in results:
        pred_class = result['predicted_class']
        class_counts[pred_class] = class_counts.get(pred_class, 0) + 1
        total_confidence += result['confidence']

    avg_confidence = total_confidence / len(results) if results else 0

    logger.info(f"\n{'=' * 60}")
    logger.info("–ò–ù–§–ï–†–ï–ù–° –ó–ê–í–ï–†–®–ï–ù")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(results)}")
    logger.info(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.4f}")
    logger.info("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for class_id in sorted(class_counts.keys()):
        count = class_counts[class_id]
        logger.info(f"  –ö–ª–∞—Å—Å {class_id}: {count} ({100 * count / len(results):.1f}%)")
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    logger.info(f"–û—Ç—á–µ—Ç: {report_path}")
    logger.info(f"{'=' * 60}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    summary_path = output_dir / "inference_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –æ–±—Ä–∞–∑—Ü–æ–≤\n")
        f.write(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.4f}\n")
        f.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n")
        for class_id in sorted(class_counts.keys()):
            count = class_counts[class_id]
            f.write(f"  –ö–ª–∞—Å—Å {class_id}: {count} ({100 * count / len(results):.1f}%)\n")

    logger.info(f"–ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {summary_path}")
    logger.info("‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()