# lightning_module.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from typing import Dict, Any, Optional
import torchmetrics
from torchmetrics import (
    Accuracy,
    F1Score,
    AUROC,
    Precision,
    Recall,
    Specificity,
    ConfusionMatrix
)
import numpy as np
from sklearn.metrics import classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns


class FocalLoss(nn.Module):
    """
    Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏.
    –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
    """

    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class MedicalClassificationModel(pl.LightningModule):
    """
    Lightning –º–æ–¥—É–ª—å –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    –í–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏, loss —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """

    def __init__(
            self,
            model: nn.Module,
            learning_rate: float = 1e-4,
            num_classes: int = 2,
            use_weighted_loss: bool = True,
            class_weights: Optional[torch.Tensor] = None,
            use_focal_loss: bool = False,
            focal_alpha: float = 1.0,
            focal_gamma: float = 2.0,
            lr_scheduler: str = 'plateau',  # 'plateau', 'cosine', 'none'
            lr_patience: int = 5,
            lr_factor: float = 0.5,
            lr_min: float = 1e-7,
            **kwargs
    ):
        super().__init__()
        self.save_hyperparameters(ignore=['model'])

        self.model = model
        self.learning_rate = learning_rate
        self.num_classes = num_classes
        self.use_weighted_loss = use_weighted_loss
        self.use_focal_loss = use_focal_loss

        if class_weights is not None:
            self.register_buffer('class_weights', class_weights)
        else:
            self.class_weights = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss_fn ‚Äî –ë–ï–ó –ø—Ä–∏–≤—è–∑–∫–∏ –∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É!
        if self.use_focal_loss:
            self.loss_fn = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
        else:
            weight = self.class_weights if self.use_weighted_loss else None
            self.loss_fn = nn.CrossEntropyLoss(weight=weight)  # weight –º–æ–∂–µ—Ç –±—ã—Ç—å None –∏–ª–∏ —Ç–µ–Ω–∑–æ—Ä–æ–º

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        self._init_metrics()
        # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ü–µ–ª–µ–π
        self.validation_step_outputs = []
        self.test_step_outputs = []

        #print(f"MedicalClassificationModel device: {self.device}", self.device)

    def _init_metrics(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫."""

        # –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫
        metric_kwargs = {
            'task': 'binary' if self.num_classes == 2 else 'multiclass',
            'num_classes': self.num_classes
        }

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.train_specificity = Specificity(**metric_kwargs)
        self.train_accuracy = Accuracy(**metric_kwargs)
        self.train_f1 = F1Score(**metric_kwargs)
        self.train_precision = Precision(**metric_kwargs)
        self.train_recall = Recall(**metric_kwargs)

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.val_accuracy = Accuracy(**metric_kwargs)
        self.val_f1 = F1Score(**metric_kwargs)
        self.val_precision = Precision(**metric_kwargs)
        self.val_recall = Recall(**metric_kwargs)
        self.val_specificity = Specificity(**metric_kwargs)
        self.val_auroc = AUROC(**metric_kwargs)

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.test_accuracy = Accuracy(**metric_kwargs)
        self.test_f1 = F1Score(**metric_kwargs)
        self.test_precision = Precision(**metric_kwargs)
        self.test_recall = Recall(**metric_kwargs)
        self.test_specificity = Specificity(**metric_kwargs)
        self.test_auroc = AUROC(**metric_kwargs)

        # Confusion Matrix
        self.val_confusion_matrix = ConfusionMatrix(**metric_kwargs)
        self.test_confusion_matrix = ConfusionMatrix(**metric_kwargs)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""
        device = next(self.model.parameters()).device
        x = x.to(device)
        return self.model(x)

    def training_step(self, batch: tuple, batch_idx: int) -> Dict[str, Any]:
        """–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è."""
        inputs, targets = batch
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # Forward pass
        outputs = self.forward(inputs)
        loss = self.loss_fn(outputs, targets)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        preds = torch.softmax(outputs, dim=1)
        pred_classes = torch.argmax(preds, dim=1)

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.train_accuracy(pred_classes, targets)
        self.train_f1(pred_classes, targets)
        self.train_precision(pred_classes, targets)
        self.train_recall(pred_classes, targets)
        self.train_specificity(pred_classes, targets)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', self.train_accuracy, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_f1', self.train_f1, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_precision', self.train_precision, on_step=False, on_epoch=True)
        self.log('train_recall', self.train_recall, on_step=False, on_epoch=True)
        self.log('train_specificity', self.train_specificity, on_step=False, on_epoch=True)

        return {
            'loss': loss,
            'preds': pred_classes.detach(),
            'targets': targets.detach(),
            'probs': preds.detach()
        }

    def validation_step(self, batch: tuple, batch_idx: int) -> Dict[str, Any]:
        """–®–∞–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        inputs, targets = batch

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # Forward pass
        outputs = self.forward(inputs)
        loss = self.loss_fn(outputs, targets)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è - —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        probs = torch.softmax(outputs, dim=1)
        pred_classes = torch.argmax(probs, dim=1)

        # –ú–µ—Ç—Ä–∏–∫–∏ - —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Ö–æ–¥—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        self.val_accuracy(pred_classes.to(self.device), targets.to(self.device))
        self.val_f1(pred_classes.to(self.device), targets.to(self.device))
        self.val_precision(pred_classes.to(self.device), targets.to(self.device))
        self.val_recall(pred_classes.to(self.device), targets.to(self.device))
        self.val_specificity(pred_classes.to(self.device), targets.to(self.device))

        # AUROC —Ç—Ä–µ–±—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if self.num_classes == 2:
            self.val_auroc(probs[:, 1].to(self.device), targets.to(self.device))
        else:
            self.val_auroc(probs.to(self.device), targets.to(self.device))

        # Confusion Matrix
        self.val_confusion_matrix(pred_classes.to(self.device), targets.to(self.device))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ - –ù–ê CPU —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –Ω–∞ GPU
        step_output = {
            'val_loss': loss.detach().cpu(),
            'preds': pred_classes.detach(),
            'targets': targets.detach(),
            'probs': probs.detach()
        }
        self.validation_step_outputs.append(step_output)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_acc', self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_precision', self.val_precision, on_step=False, on_epoch=True, prog_bar=False)
        self.log('val_recall', self.val_recall, on_step=False, on_epoch=True, prog_bar=False)
        self.log('val_specificity', self.val_specificity, on_step=False, on_epoch=True, prog_bar=False)

        return step_output

    def test_step(self, batch: tuple, batch_idx: int) -> Dict[str, Any]:
        """–®–∞–≥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        inputs, targets = batch
        device = next(self.model.parameters()).device
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # Forward pass
        outputs = self.forward(inputs)
        loss = self.loss_fn(outputs, targets)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        probs = torch.softmax(outputs, dim=1)
        pred_classes = torch.argmax(probs, dim=1)

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.test_accuracy(pred_classes, targets)
        self.test_f1(pred_classes, targets)
        self.test_precision(pred_classes, targets)
        self.test_recall(pred_classes, targets)
        self.test_specificity(pred_classes, targets)

        # AUROC
        if self.num_classes == 2:
            self.test_auroc(probs[:, 1], targets)
        else:
            self.test_auroc(probs, targets)

        # Confusion Matrix
        self.test_confusion_matrix(pred_classes, targets)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ù–ê –¢–û–ú –ñ–ï –£–°–¢–†–û–ô–°–¢–í–ï
        step_output = {
            'test_loss': loss,
            'preds': pred_classes.detach(),  # –£–±–∏—Ä–∞–µ–º .cpu()
            'targets': targets.detach(),     # –£–±–∏—Ä–∞–µ–º .cpu()
            'probs': probs.detach()          # –£–±–∏—Ä–∞–µ–º .cpu()
        }
        self.test_step_outputs.append(step_output)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log('test_loss', loss, on_step=False, on_epoch=True)
        self.log('test_acc', self.test_accuracy, on_step=False, on_epoch=True)
        self.log('test_f1', self.test_f1, on_step=False, on_epoch=True)
        self.log('test_precision', self.test_precision, on_step=False, on_epoch=True)
        self.log('test_recall', self.test_recall, on_step=False, on_epoch=True)
        self.log('test_specificity', self.test_specificity, on_step=False, on_epoch=True)


        return step_output

    def on_validation_epoch_end(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        if self.validation_step_outputs:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
            device = self.device  # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.device –≤–º–µ—Å—Ç–æ next(self.model.parameters()).device
            all_preds = torch.cat([x['preds'].to(device) for x in self.validation_step_outputs])
            all_targets = torch.cat([x['targets'].to(device) for x in self.validation_step_outputs])
            all_probs = torch.cat([x['probs'].to(device) for x in self.validation_step_outputs])

            # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç (–∫–∞–∂–¥—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö)
            if self.current_epoch % 10 == 0:
                self._log_detailed_metrics(all_preds.cpu(), all_targets.cpu(), all_probs.cpu(), 'val')

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–ª–ª–±—ç–∫–∞—Ö
            self.validation_metrics = {
                'val_acc': self.val_accuracy.compute().item(),
                'val_f1': self.val_f1.compute().item(),
                'val_auroc': self.val_auroc.compute().item(),
                'val_precision': self.val_precision.compute().item(),
                'val_recall': self.val_recall.compute().item(),
                'val_specificity': self.val_specificity.compute().item()
            }

            # –û—á–∏—â–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã
            self.validation_step_outputs.clear()

    def on_test_epoch_end(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if self.test_step_outputs:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
            device = self.device  # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.device –≤–º–µ—Å—Ç–æ next(self.model.parameters()).device
            all_preds = torch.cat([x['preds'].to(device) for x in self.test_step_outputs])
            all_targets = torch.cat([x['targets'].to(device) for x in self.test_step_outputs])
            all_probs = torch.cat([x['probs'].to(device) for x in self.test_step_outputs])

            # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            self._log_detailed_metrics(all_preds.cpu(), all_targets.cpu(), all_probs.cpu(), 'test')

            # –û—á–∏—â–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã
            self.test_step_outputs.clear()

    def _log_detailed_metrics(self, preds: torch.Tensor, targets: torch.Tensor,
                              probs: torch.Tensor, stage: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        try:
            report = classification_report(
                targets.numpy(),
                preds.numpy(),
                target_names=['–ù–æ—Ä–º–∞', '–ü–∞—Ç–æ–ª–æ–≥–∏—è'] if self.num_classes == 2 else None,
                output_dict=True,
                zero_division=0
            )

            # –õ–æ–≥–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            if self.logger:
                self.logger.experiment.add_text(
                    f'{stage}_classification_report',
                    str(classification_report(
                        targets.numpy(),
                        preds.numpy(),
                        target_names=['–ù–æ—Ä–º–∞', '–ü–∞—Ç–æ–ª–æ–≥–∏—è'] if self.num_classes == 2 else None,
                        zero_division=0
                    )),
                    self.current_epoch
                )
        except Exception as e:
            self.print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")

        # Confusion Matrix –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        try:
            cm = self.val_confusion_matrix.compute() if stage == 'val' else self.test_confusion_matrix.compute()
            cm_np = cm.cpu().numpy()

            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(
                cm_np,
                annot=True,
                fmt='d',
                cmap='Blues',
                xticklabels=['–ù–æ—Ä–º–∞', '–ü–∞—Ç–æ–ª–æ–≥–∏—è'] if self.num_classes == 2 else None,
                yticklabels=['–ù–æ—Ä–º–∞', '–ü–∞—Ç–æ–ª–æ–≥–∏—è'] if self.num_classes == 2 else None,
                ax=ax
            )
            ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
            ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
            ax.set_title(f'Confusion Matrix ({stage.upper()})')

            if self.logger:
                self.logger.experiment.add_figure(f'{stage}_confusion_matrix', fig, self.current_epoch)

            plt.close(fig)

        except Exception as e:
            self.print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ confusion matrix: {e}")

        # ROC –∫—Ä–∏–≤–∞—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.num_classes == 2:
            try:
                fpr, tpr, _ = roc_curve(targets.numpy(), probs[:, 1].numpy())
                roc_auc = auc(fpr, tpr)

                fig, ax = plt.subplots(figsize=(8, 6))
                ax.plot(fpr, tpr, color='darkorange', lw=2,
                        label=f'ROC –∫—Ä–∏–≤–∞—è (AUC = {roc_auc:.3f})')
                ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
                        label='–°–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä')
                ax.set_xlim([0.0, 1.0])
                ax.set_ylim([0.0, 1.05])
                ax.set_xlabel('False Positive Rate')
                ax.set_ylabel('True Positive Rate')
                ax.set_title(f'ROC –∫—Ä–∏–≤–∞—è ({stage.upper()})')
                ax.legend(loc="lower right")
                ax.grid(True, alpha=0.3)

                if self.logger:
                    self.logger.experiment.add_figure(f'{stage}_roc_curve', fig, self.current_epoch)

                plt.close(fig)

            except Exception as e:
                self.print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ ROC –∫—Ä–∏–≤–æ–π: {e}")

    def configure_optimizers(self):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞."""

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam —Å weight decay
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-5,
            betas=(0.9, 0.999)
        )

        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
        if self.hparams.lr_scheduler == 'plateau':
            scheduler = ReduceLROnPlateau(
                optimizer,
                mode='max',  # –¥–ª—è F1 score
                factor=self.hparams.lr_factor,
                patience=self.hparams.lr_patience,
                min_lr=self.hparams.lr_min,
                verbose=True
            )

            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "monitor": "val_f1",
                    "interval": "epoch",
                    "frequency": 1,
                }
            }

        elif self.hparams.lr_scheduler == 'cosine':
            scheduler = CosineAnnealingLR(
                optimizer,
                T_max=25,  # –ø–æ–ª–æ–≤–∏–Ω–∞ –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ —ç–ø–æ—Ö
                eta_min=self.hparams.lr_min
            )

            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1,
                }
            }

        else:
            return optimizer

    def predict_step(self, batch: tuple, batch_idx: int) -> Dict[str, torch.Tensor]:
        """–®–∞–≥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è inference."""
        inputs, _ = batch if isinstance(batch, (tuple, list)) and len(batch) == 2 else (batch, None)

        outputs = self.forward(inputs)
        probs = torch.softmax(outputs, dim=1)
        pred_classes = torch.argmax(probs, dim=1)

        return {
            'predictions': pred_classes,
            'probabilities': probs,
            'logits': outputs
        }

    def get_model_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        summary = f"""
        üè• Medical Classification Model Summary:
        ‚Ä¢ Total parameters: {total_params:,}
        ‚Ä¢ Trainable parameters: {trainable_params:,}
        ‚Ä¢ Non-trainable parameters: {total_params - trainable_params:,}
        ‚Ä¢ Model: {self.model.__class__.__name__}
        ‚Ä¢ Loss function: {'Focal Loss' if self.use_focal_loss else 'Cross Entropy'}
        ‚Ä¢ Weighted loss: {self.use_weighted_loss}
        ‚Ä¢ Learning rate: {self.learning_rate}
        ‚Ä¢ Number of classes: {self.num_classes}
        """

        return summary
