# config.py
from dataclasses import dataclass, field
from typing import List, Optional, Tuple

import os
from dotenv import load_dotenv
load_dotenv()

@dataclass
class ModelConfig:
    # ========== MODEL ARCHITECTURE ==========
    model: str = "resnet"
    model_depth: int = 34
    resnet_shortcut: str = "A"

    # ========== INPUT DIMENSIONS ==========
    input_W: int = 128  # –∏–∑–º–µ–Ω–µ–Ω–æ —Å 128
    input_H: int = 128  # –∏–∑–º–µ–Ω–µ–Ω–æ —Å 128
    input_D: int = 128  # –∏–∑–º–µ–Ω–µ–Ω–æ —Å 128

    # ========== CLASSIFICATION PARAMETERS ==========
    n_seg_classes: int = 2  # –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    binary_classification: bool = True

    # ========== TRAINING HYPERPARAMETERS ==========
    batch_size: int = os.getenv("MODEL_HP_BATCH_SIZE") or 1
    learning_rate: float = 1e-4
    n_epochs: int = os.getenv("MODEL_HP_EPOCHS") or 1
    num_workers: int = os.getenv("MODEL_HP_WORKERS") or 0

    # ========== ADVANCED TRAINING PARAMETERS ==========
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    weight_decay: float = 1e-5
    gradient_clip_val: float = 1.0

    # Learning Rate Scheduler
    lr_scheduler: str = "plateau"  # "plateau", "cosine", "none"
    lr_scheduler_patience: int = 7
    lr_scheduler_factor: float = 0.5
    lr_scheduler_min_lr: float = 1e-7

    # ========== LOSS FUNCTION PARAMETERS ==========
    # –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    use_focal_loss: bool = False  # True –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    focal_alpha: float = 1.0
    focal_gamma: float = 2.0

    # Weighted Loss –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    use_weighted_loss: bool = False
    auto_class_weights: bool = True  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤

    # ========== EARLY STOPPING ==========
    early_stopping_patience: int = 25  # —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    early_stopping_min_delta: float = 0.001
    early_stopping_metric: str = "val_f1"

    # ========== CHECKPOINTING ==========
    save_intervals: int = 10
    save_top_k: int = 3
    monitor_metric: str = "val_f1"
    checkpoint_mode: str = "max"
    save_weights_only: bool = False  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

    # ========== CROSS-VALIDATION ==========
    use_cross_validation: bool = True
    n_splits: int = 5
    cv_random_state: int = 42
    stratified_cv: bool = True  # —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è

    # ========== DATA PATHS ==========
    data_root: str = "data/train"
    img_list: str = "data/train/labels.csv"
    val_data_root: str = "data/test"
    val_list: str = "data/test/labels.csv"

    # –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    pretrain_path: str = "model/pretrain/resnet_34_23dataset.pth"
    use_pretrained: bool = True

    # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    save_folder: str = "model/outputs/checkpoints"
    log_folder: str = "logs"
    tb_log_folder: str = "tb_logs"

    # ========== DATA AUGMENTATION ==========
    # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    aug_flip_prob: float = 0.3
    aug_rotate_prob: float = 0.2

    # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    aug_noise_prob: float = 0.15
    aug_noise_std: float = 0.005
    aug_intensity_shift_prob: float = 0.2
    aug_intensity_shift_offset: float = 0.05
    aug_contrast_prob: float = 0.2
    aug_contrast_gamma: Tuple[float, float] = (0.9, 1.1)
    aug_scale_intensity_prob: float = 0.2
    aug_scale_intensity_factors: Tuple[float, float] = (-0.05, 0.05)

    # ========== MODEL FINE-TUNING ==========
    # –°–ª–æ–∏ –¥–ª—è —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è/–æ–±—É—á–µ–Ω–∏—è
    new_layer_names: List[str] = field(default_factory=lambda: ["fc", "layer4"])

    # –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ learning rates
    use_differential_lr: bool = True
    base_lr_multiplier: float = 0.1  # –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤
    new_layers_lr_multiplier: float = 1.0  # –¥–ª—è –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤

    # ========== METRICS AND LOGGING ==========
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    primary_metric: str = "val_f1"
    log_every_n_steps: int = 10

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    track_additional_metrics: bool = True
    additional_metrics: List[str] = field(default_factory=lambda: [
        "accuracy", "precision", "recall", "specificity", "auroc"
    ])

    # –ß–∞—Å—Ç–æ—Ç–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    detailed_logging_frequency: int = 10  # –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö

    # ========== MEDICAL-SPECIFIC PARAMETERS ==========
    # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    calculate_sensitivity: bool = True  # recall –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—ã
    calculate_specificity: bool = True
    calculate_ppv: bool = True  # –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
    calculate_npv: bool = True  # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å

    # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    classification_threshold: float = 0.5
    optimize_threshold: bool = True  # –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

    # ========== SYSTEM SETTINGS ==========
    no_cuda: bool = False
    manual_seed: int = 42
    deterministic: bool = False
    pin_memory: bool = True

    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    mixed_precision: bool = True  # 16-bit training
    compile_model: bool = os.getenv('MODEL_HP_COMPILE_MODEL') or False  # PyTorch 2.0 compilation

    # ========== TESTING AND DEBUGGING ==========
    ci_test: bool = False
    fast_dev_run: bool = False
    overfit_batches: float = 0.0  # –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

    # –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    profiler: Optional[str] = None  # "simple", "advanced", "pytorch"

    # ========== INFERENCE PARAMETERS ==========
    # Test-Time Augmentation
    use_tta: bool = False
    tta_transforms: int = 8

    # Ensemble –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    use_ensemble: bool = False
    ensemble_models: List[str] = field(default_factory=list)

    print(f"Learning rate: {learning_rate}")
    print(f"Batch size: {batch_size}")
    print(f"Epochs: {n_epochs}")
    print(f"Num workers: {num_workers}")

    def __post_init__(self):
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if self.n_seg_classes != 2 and self.binary_classification:
            self.binary_classification = False
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: binary_classification —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ False –∏–∑-–∑–∞ n_seg_classes != 2")

        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ä–∞–∑–º–µ—Ä–µ –≤—Ö–æ–¥–∞
        if (self.input_W, self.input_H, self.input_D) != (128, 128, 128):
            print(f"‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {self.input_W}√ó{self.input_H}√ó{self.input_D}")
            print("    –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –ø–µ—Ä–µ—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.binary_classification:
            if "auroc" not in self.additional_metrics:
                self.additional_metrics.append("auroc")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π
        from pathlib import Path

        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        Path(self.save_folder).mkdir(parents=True, exist_ok=True)
        Path(self.log_folder).mkdir(exist_ok=True)
        Path(self.tb_log_folder).mkdir(exist_ok=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è CI —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if self.ci_test:
            self.n_epochs = 2
            self.batch_size = 2
            self.num_workers = 0
            self.n_splits = 2
            self.early_stopping_patience = 3
            self.save_top_k = 1
            self.fast_dev_run = True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ scheduler
        valid_schedulers = ["plateau", "cosine", "none"]
        if self.lr_scheduler not in valid_schedulers:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π scheduler '{self.lr_scheduler}'. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'plateau'.")
            self.lr_scheduler = "plateau"

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ focal loss –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if self.use_focal_loss and self.binary_classification:
            # –î–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ gamma=2, alpha –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
            if not hasattr(self, '_focal_configured'):
                print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤")
                self._focal_configured = True

    def get_augmentation_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º."""
        aug_summary = f"""
        üîÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π:
        ‚Ä¢ Flip: {self.aug_flip_prob:.2f}
        ‚Ä¢ Rotate: {self.aug_rotate_prob:.2f}  
        ‚Ä¢ Noise: {self.aug_noise_prob:.2f} (std={self.aug_noise_std:.3f})
        ‚Ä¢ Intensity shift: {self.aug_intensity_shift_prob:.2f}
        ‚Ä¢ Contrast: {self.aug_contrast_prob:.2f}
        ‚Ä¢ Scale intensity: {self.aug_scale_intensity_prob:.2f}
        """
        return aug_summary

    def get_training_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ–±—É—á–µ–Ω–∏—é."""
        loss_type = "Focal Loss" if self.use_focal_loss else "Cross Entropy"
        if self.use_weighted_loss:
            loss_type += " (Weighted)"

        training_summary = f"""
        üéØ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:
        ‚Ä¢ Batch size: {self.batch_size}
        ‚Ä¢ Learning rate: {self.learning_rate}
        ‚Ä¢ Max epochs: {self.n_epochs}
        ‚Ä¢ Loss function: {loss_type}
        ‚Ä¢ LR Scheduler: {self.lr_scheduler}
        ‚Ä¢ Early stopping: {self.early_stopping_patience} epochs
        ‚Ä¢ Cross-validation: {self.n_splits} folds
        ‚Ä¢ Primary metric: {self.primary_metric}
        """
        return training_summary

    def validate_config(self) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π."""
        warnings = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        if self.batch_size < 4:
            warnings.append("–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π batch_size –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ BatchNorm")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ learning rate
        if self.learning_rate > 1e-2:
            warnings.append("–í—ã—Å–æ–∫–∏–π learning_rate –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")

        if self.learning_rate < 1e-6:
            warnings.append("–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π learning_rate –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ early stopping
        if self.early_stopping_patience < 5:
            warnings.append("–ù–∏–∑–∫–∏–π early_stopping_patience –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–µ")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
        from pathlib import Path
        if not Path(self.img_list).exists():
            warnings.append(f"–§–∞–π–ª –º–µ—Ç–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.img_list}")

        if self.use_pretrained and self.pretrain_path and not Path(self.pretrain_path).exists():
            warnings.append(f"–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.pretrain_path}")

        return warnings
