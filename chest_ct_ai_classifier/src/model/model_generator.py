import torch
from torch import nn
from .models import resnet


def adapt_model_for_input_size(model, input_size, model_depth, n_seg_classes):
    """
    ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ñ.
    """
    print(f"ğŸ”§ ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ {input_size}...")

    # Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    print("â„ï¸ Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²...")
    for param in model.parameters():
        param.requires_grad = False

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    device = next(model.parameters()).device
    print(f"ğŸ¯ Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {device}")

    # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
    with torch.no_grad():
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ dummy_input Ğ½Ğ° Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        dummy_input = torch.randn(1, 1, input_size[2], input_size[1], input_size[0]).to(device)

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        if hasattr(model, 'module'):
            # DataParallel ÑĞ»ÑƒÑ‡Ğ°Ğ¹
            conv_features = nn.Sequential(
                model.module.conv1,
                model.module.bn1,
                model.module.relu,
                model.module.maxpool,
                model.module.layer1,
                model.module.layer2,
                model.module.layer3,
                model.module.layer4,
                model.module.avgpool
            )
        else:
            conv_features = nn.Sequential(
                model.conv1,
                model.bn1,
                model.relu,
                model.maxpool,
                model.layer1,
                model.layer2,
                model.layer3,
                model.layer4,
                model.avgpool
            )

        # ĞŸĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµĞ¼ Sequential Ğ½Ğ° Ñ‚Ğ¾ Ğ¶Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
        conv_features = conv_features.to(device)

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ğº
        conv_output = conv_features(dummy_input)
        flattened_size = conv_output.view(conv_output.size(0), -1).size(1)

    print(f"ğŸ“Š ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ…Ğ¾Ğ´Ğ° FC ÑĞ»Ğ¾Ñ: {flattened_size}")

    # Ğ—Ğ°Ğ¼ĞµĞ½Ğ° FC ÑĞ»Ğ¾Ñ
    if hasattr(model, 'module'):
        old_fc = model.module.fc
        model.module.fc = nn.Linear(flattened_size, n_seg_classes)
        new_fc = model.module.fc
    else:
        old_fc = model.fc
        model.fc = nn.Linear(flattened_size, n_seg_classes)
        new_fc = model.fc

    # ĞŸĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ FC ÑĞ»Ğ¾Ğ¹ Ğ½Ğ° Ñ‚Ğ¾ Ğ¶Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
    if hasattr(model, 'module'):
        model.module.fc = model.module.fc.to(device)
    else:
        model.fc = model.fc.to(device)

    print(f"ğŸ”„ Ğ—Ğ°Ğ¼ĞµĞ½ĞµĞ½ FC ÑĞ»Ğ¾Ğ¹: {old_fc.in_features} â†’ {flattened_size} Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ², {n_seg_classes} Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ²")

    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
    if isinstance(new_fc, nn.Linear):
        nn.init.xavier_uniform_(new_fc.weight)
        if new_fc.bias is not None:
            nn.init.zeros_(new_fc.bias)

    # Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ FC ÑĞ»Ğ¾Ñ
    print("ğŸ”¥ Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ FC ÑĞ»Ğ¾Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...")
    for param in new_fc.parameters():
        param.requires_grad = True
        # 5. Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ layer3 Ğ¸ layer4
        print("ğŸ”¥ Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ layer3 Ğ¸ layer4...")
        if hasattr(model, 'module'):
            for p in model.module.layer3.parameters():
                p.requires_grad = True
            for p in model.module.layer4.parameters():
                p.requires_grad = True
        else:
            for p in model.layer3.parameters():
                p.requires_grad = True
            for p in model.layer4.parameters():
                p.requires_grad = True

    # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    trainable_parameters = list(filter(lambda p: p.requires_grad, model.parameters()))

    return model, trainable_parameters


def generate_model(opt):
    assert opt.model in ['resnet']

    if opt.model == 'resnet':
        assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸
        model_functions = {
            10: resnet.resnet10,
            18: resnet.resnet18,
            34: resnet.resnet34,
            50: resnet.resnet50,
            101: resnet.resnet101,
            152: resnet.resnet152,
            200: resnet.resnet200
        }

        model = model_functions[opt.model_depth](
            sample_input_W=opt.input_W,
            sample_input_H=opt.input_H,
            sample_input_D=opt.input_D,
            shortcut_type=opt.resnet_shortcut,
            no_cuda=opt.no_cuda,
            num_seg_classes=opt.n_seg_classes
        )

    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ»Ñ GPU/CPU
    if not opt.no_cuda:
        if len(opt.gpu_id) > 1:
            model = model.cuda()
            model = nn.DataParallel(model, device_ids=opt.gpu_id)
            net_dict = model.state_dict()
        else:
            import os
            os.environ["CUDA_VISIBLE_DEVICES"] = str(opt.gpu_id[0])
            model = model.cuda()
            model = nn.DataParallel(model, device_ids=None)
            net_dict = model.state_dict()
    else:
        net_dict = model.state_dict()

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    if opt.phase != 'test' and opt.pretrain_path:
        print('ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {}'.format(opt.pretrain_path))

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ CPU/GPU
        if opt.no_cuda or not torch.cuda.is_available():
            pretrain = torch.load(opt.pretrain_path, weights_only=True, map_location=torch.device('cpu'))
        else:
            pretrain = torch.load(opt.pretrain_path, weights_only=True)

        pretrain_dict = {k: v for k, v in pretrain['state_dict'].items() if k in net_dict.keys()}
        net_dict.update(pretrain_dict)
        model.load_state_dict(net_dict, strict=False) # Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ Ğ² Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²
        current_input_size = (opt.input_W, opt.input_H, opt.input_D)
        pretrained_input_size = (128, 128, 128)  # Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

        if current_input_size != pretrained_input_size:
            print(f"âš ï¸ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ°: {pretrained_input_size} â†’ {current_input_size}")
            print("ğŸ”§ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")

            # ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            model, adapted_parameters = adapt_model_for_input_size(
                model, current_input_size, opt.model_depth, opt.n_seg_classes
            )

            print("âœ… ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
            return model, adapted_parameters

        # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²
        new_parameters = []
        for pname, p in model.named_parameters():
            for layer_name in opt.new_layer_names:
                if pname.find(layer_name) >= 0:
                    new_parameters.append(p)
                    break

        new_parameters_id = list(map(id, new_parameters))
        base_parameters = list(filter(lambda p: id(p) not in new_parameters_id, model.parameters()))
        parameters = {'base_parameters': base_parameters,
                      'new_parameters': new_parameters}

        return model, parameters

    return model, model.parameters()
