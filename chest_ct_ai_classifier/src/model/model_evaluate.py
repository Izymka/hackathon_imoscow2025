import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, \
    average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
from monai.metrics import compute_roc_auc
from monai.metrics import ConfusionMatrixMetric
import os
from tqdm import tqdm
import json


def evaluate_model(model, test_loader, device, class_names=None, save_results=True, output_dir='results'):
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á

    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        test_loader: DataLoader —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
        class_names: —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
        save_results: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """

    if save_results:
        os.makedirs(output_dir, exist_ok=True)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
    if device is None:
        device = next(model.parameters()).device

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ DataParallel –º–æ–¥–µ–ª–µ–π
    if isinstance(model, torch.nn.DataParallel):
        model = model.module

    model = model.to(device)

    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
    model.eval()

    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_predictions = []
    all_probabilities = []
    all_labels = []
    all_filenames = []

    # –ö—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss
    criterion = nn.CrossEntropyLoss()
    total_loss = 0.0

    print("–ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏...")

    with torch.no_grad():
        for batch_idx, (data, targets) in enumerate(tqdm(test_loader, desc="–û—Ü–µ–Ω–∫–∞")):
            data, targets = data.to(device), targets.to(device)

            # –ü—Ä–æ–≥–Ω–æ–∑
            outputs = model(data)
            loss = criterion(outputs, targets)
            total_loss += loss.item()

            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            probabilities = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs.data, 1)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            all_labels.extend(targets.cpu().numpy())

            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è GPU
            if device.type == 'cuda':
                torch.cuda.empty_cache()

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_probabilities = np.array(all_probabilities)

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    accuracy = np.mean(all_predictions == all_labels)
    avg_loss = total_loss / len(test_loader)

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
    if class_names is None:
        class_names = [f"Class_{i}" for i in range(len(np.unique(all_labels)))]

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(all_labels, all_predictions)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    additional_metrics = {}
    if len(class_names) == 2:
        tn, fp, fn, tp = cm.ravel()

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (Recall)
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # –¢–æ—á–Ω–æ—Å—Ç—å (Precision)

        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0.0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0  # Negative Predictive Value
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0  # False Positive Rate
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0  # False Negative Rate

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è
        plr = sensitivity / (1 - specificity) if (1 - specificity) > 0 else float('inf')  # Positive Likelihood Ratio
        nlr = (1 - sensitivity) / specificity if specificity > 0 else float('inf')  # Negative Likelihood Ratio

        # AUC-ROC
        auc_score = roc_auc_score(all_labels, all_probabilities[:, 1])

        # Average Precision
        avg_precision = average_precision_score(all_labels, all_probabilities[:, 1])

        additional_metrics = {
            'sensitivity': sensitivity,
            'specificity': specificity,
            'precision': precision,
            'f1_score': f1_score,
            'npv': npv,
            'fpr': fpr,
            'fnr': fnr,
            'plr': plr,
            'nlr': nlr,
            'auc_roc': auc_score,
            'average_precision': avg_precision,
            'true_positive': int(tp),
            'true_negative': int(tn),
            'false_positive': int(fp),
            'false_negative': int(fn)
        }

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n{'=' * 60}")
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
    print(f"{'=' * 60}")
    print(f"üìä –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"üìà –°—Ä–µ–¥–Ω–∏–π Loss: {avg_loss:.4f}")

    if len(class_names) == 2:
        print(f"\nü©∫ –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):")
        print(f"  ‚Ä¢ –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (Sensitivity/Recall): {sensitivity:.4f} ({sensitivity * 100:.2f}%)")
        print(f"  ‚Ä¢ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å (Specificity): {specificity:.4f} ({specificity * 100:.2f}%)")
        print(f"  ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Precision/PPV): {precision:.4f} ({precision * 100:.2f}%)")
        print(f"  ‚Ä¢ F1-Score: {f1_score:.4f} ({f1_score * 100:.2f}%)")
        print(f"  ‚Ä¢ NPV: {npv:.4f} ({npv * 100:.2f}%)")
        print(f"  ‚Ä¢ AUC-ROC: {auc_score:.4f}")
        print(f"  ‚Ä¢ Average Precision: {avg_precision:.4f}")
        print(f"\nüìã –ö–û–ù–¢–ò–ù–ì–ï–ù–¢–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:")
        print(f"  ‚Ä¢ True Positive (TP): {tp}")
        print(f"  ‚Ä¢ True Negative (TN): {tn}")
        print(f"  ‚Ä¢ False Positive (FP): {fp}")
        print(f"  ‚Ä¢ False Negative (FN): {fn}")

    # –ü–æ–¥—Ä–æ–±–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\nüìã –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    print(classification_report(all_labels, all_predictions, target_names=class_names, digits=4))

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if save_results:
        save_evaluation_results(
            all_labels, all_predictions, all_probabilities,
            cm, accuracy, avg_loss, class_names, output_dir, additional_metrics
        )

    return {
        'accuracy': accuracy,
        'loss': avg_loss,
        'predictions': all_predictions,
        'labels': all_labels,
        'probabilities': all_probabilities,
        'confusion_matrix': cm,
        'additional_metrics': additional_metrics
    }


def save_evaluation_results(labels, predictions, probabilities, cm, accuracy, loss, class_names, output_dir,
                            additional_metrics=None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏"""

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ JSON —Ñ–∞–π–ª
    metrics_dict = {
        'accuracy': float(accuracy),
        'loss': float(loss),
        'confusion_matrix': cm.tolist(),
        'class_names': class_names
    }

    if additional_metrics:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy –∑–Ω–∞—á–µ–Ω–∏—è –≤ float –¥–ª—è JSON
        for key, value in additional_metrics.items():
            if isinstance(value, (np.float32, np.float64)):
                metrics_dict[key] = float(value)
            else:
                metrics_dict[key] = value

    with open(os.path.join(output_dir, 'evaluation_metrics.json'), 'w', encoding='utf-8') as f:
        json.dump(metrics_dict, f, indent=4, ensure_ascii=False)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    with open(os.path.join(output_dir, 'evaluation_metrics.txt'), 'w', encoding='utf-8') as f:
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò\n")
        f.write("=" * 60 + "\n")
        f.write(f"–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f}\n")
        f.write(f"–°—Ä–µ–¥–Ω–∏–π Loss: {loss:.4f}\n\n")

        if additional_metrics:
            f.write("–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò:\n")
            f.write(f"  –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (Sensitivity): {additional_metrics.get('sensitivity', 0):.4f}\n")
            f.write(f"  –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å (Specificity): {additional_metrics.get('specificity', 0):.4f}\n")
            f.write(f"  –¢–æ—á–Ω–æ—Å—Ç—å (Precision): {additional_metrics.get('precision', 0):.4f}\n")
            f.write(f"  F1-Score: {additional_metrics.get('f1_score', 0):.4f}\n")
            f.write(f"  NPV: {additional_metrics.get('npv', 0):.4f}\n")
            f.write(f"  AUC-ROC: {additional_metrics.get('auc_roc', 0):.4f}\n")
            f.write(f"  Average Precision: {additional_metrics.get('average_precision', 0):.4f}\n\n")

        f.write("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:\n")
        f.write(classification_report(labels, predictions, target_names=class_names))

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–µ–≤'})
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    results_df = pd.DataFrame({
        'true_label': labels,
        'predicted_label': predictions,
        'true_label_name': [class_names[i] for i in labels],
        'predicted_label_name': [class_names[i] for i in predictions]
    })

    # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    for i, class_name in enumerate(class_names):
        results_df[f'probability_{class_name}'] = probabilities[:, i]

    results_df.to_csv(os.path.join(output_dir, 'predictions.csv'), index=False, encoding='utf-8')

    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {output_dir}")


def plot_roc_curve(labels, probabilities, class_names, output_dir='results'):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-–∫—Ä–∏–≤–æ–π –∏ Precision-Recall –∫—Ä–∏–≤–æ–π"""

    plt.figure(figsize=(15, 6))

    # ROC-–∫—Ä–∏–≤–∞—è
    plt.subplot(1, 2, 1)

    if len(class_names) == 2:
        # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])
        auc_score = roc_auc_score(labels, probabilities[:, 1])
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                 label=f'ROC Curve (AUC = {auc_score:.4f})')

        # Precision-Recall –∫—Ä–∏–≤–∞—è
        plt.subplot(1, 2, 2)
        precision, recall, _ = precision_recall_curve(labels, probabilities[:, 1])
        avg_precision = average_precision_score(labels, probabilities[:, 1])
        plt.plot(recall, precision, color='blue', lw=2,
                 label=f'PR Curve (AP = {avg_precision:.4f})')

    else:
        # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        colors = plt.cm.Set3(np.linspace(0, 1, len(class_names)))

        # ROC-–∫—Ä–∏–≤—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        plt.subplot(1, 2, 1)
        for i, (class_name, color) in enumerate(zip(class_names, colors)):
            try:
                fpr, tpr, _ = roc_curve(labels == i, probabilities[:, i])
                auc_score = roc_auc_score(labels == i, probabilities[:, i])
                plt.plot(fpr, tpr, color=color, lw=2,
                         label=f'{class_name} (AUC = {auc_score:.4f})')
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ ROC –¥–ª—è –∫–ª–∞—Å—Å–∞ {class_name}: {e}")
                continue

        # Precision-Recall –∫—Ä–∏–≤—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        plt.subplot(1, 2, 2)
        for i, (class_name, color) in enumerate(zip(class_names, colors)):
            try:
                precision, recall, _ = precision_recall_curve(labels == i, probabilities[:, i])
                avg_precision = average_precision_score(labels == i, probabilities[:, i])
                plt.plot(recall, precision, color=color, lw=2,
                         label=f'{class_name} (AP = {avg_precision:.4f})')
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ PR –¥–ª—è –∫–ª–∞—Å—Å–∞ {class_name}: {e}")
                continue

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ROC subplot
    plt.subplot(1, 2, 1)
    plt.plot([0, 1], [0, 1], 'k--', label='–°–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä', alpha=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Precision-Recall subplot
    plt.subplot(1, 2, 2)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'roc_pr_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()


def calculate_confidence_intervals(metrics, labels, predictions, n_bootstrap=1000):
    """
    –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ—Ç–æ–¥–æ–º –±—É—Ç—Å—Ç—Ä—ç–ø

    Args:
        metrics: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        labels: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        n_bootstrap: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–æ–∫
    """
    n_samples = len(labels)
    bootstrapped_metrics = {
        'accuracy': [],
        'sensitivity': [],
        'specificity': []
    }

    for _ in range(n_bootstrap):
        # –°–æ–∑–¥–∞–µ–º –±—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–∫—É
        indices = np.random.choice(n_samples, n_samples, replace=True)
        boot_labels = labels[indices]
        boot_preds = predictions[indices]

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–∫–∏
        bootstrapped_metrics['accuracy'].append(np.mean(boot_labels == boot_preds))

        if len(np.unique(labels)) == 2:
            cm = confusion_matrix(boot_labels, boot_preds)
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                bootstrapped_metrics['sensitivity'].append(sensitivity)
                bootstrapped_metrics['specificity'].append(specificity)

    # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (95%)
    ci_metrics = {}
    for metric_name, values in bootstrapped_metrics.items():
        if values:
            ci_metrics[metric_name] = {
                'mean': np.mean(values),
                'ci_lower': np.percentile(values, 2.5),
                'ci_upper': np.percentile(values, 97.5)
            }

    return ci_metrics


def quick_evaluate(model_path, test_dataset, device='cpu', batch_size=1, class_names=None):
    """
    –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏

    Args:
        model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        test_dataset: —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        class_names: —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
    """

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    checkpoint = torch.load(model_path, map_location=device)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —á—Ç–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å - –º–æ–¥–µ–ª—å –∏–ª–∏ state_dict
    if 'model' in checkpoint:
        model = checkpoint['model']
    elif 'state_dict' in checkpoint:
        # –ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        # –≠—Ç–æ –º–µ—Å—Ç–æ –¥–ª—è –≤–∞—à–µ–π –∫–∞—Å—Ç–æ–º–Ω–æ–π –ª–æ–≥–∏–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        model = None  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à—É –º–æ–¥–µ–ª—å
        model.load_state_dict(checkpoint['state_dict'])
    else:
        model = checkpoint  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω —Å–∞–º model

    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if class_names is None:
        class_names = ['Normal', 'Abnormal']

    # –û—Ü–µ–Ω–∫–∞
    results = evaluate_model(
        model=model,
        test_loader=test_loader,
        device=device,
        class_names=class_names
    )

    return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è Jupyter –Ω–æ—É—Ç–±—É–∫–∞
def demo_usage():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–π"""
    print("üî¨ –ú–æ–¥—É–ª—å –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω")
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:")
    print("  ‚Ä¢ evaluate_model() - –ø–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏")
    print("  ‚Ä¢ plot_roc_curve() - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC –∏ PR –∫—Ä–∏–≤—ã—Ö")
    print("  ‚Ä¢ quick_evaluate() - –±—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞")
    print("  ‚Ä¢ calculate_confidence_intervals() - –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã")

    print("\nüìù –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Jupyter –Ω–æ—É—Ç–±—É–∫–µ:")
    print("""
# 1. –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π
from evaluate_model import evaluate_model, plot_roc_curve

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
# model = ... –≤–∞—à–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
# test_loader = ... –≤–∞—à DataLoader

# 3. –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
results = evaluate_model(
    model=model,
    test_loader=test_loader,
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
    class_names=['–ó–¥–æ—Ä–æ–≤', '–ë–æ–ª–µ–Ω'],
    save_results=True,
    output_dir='evaluation_results'
)

# 4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
plot_roc_curve(
    results['labels'],
    results['probabilities'],
    ['–ó–¥–æ—Ä–æ–≤', '–ë–æ–ª–µ–Ω'],
    output_dir='evaluation_results'
)
    """)


# –£—Å–ª–æ–≤–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ (—É–±—Ä–∞—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã)
if __name__ == "__main__":
    demo_usage()

    # –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ:
    """
    try:
        # –ü—Ä–∏–º–µ—Ä —Å —É—Å–ª–æ–≤–Ω—ã–º –∏–º–ø–æ—Ä—Ç–æ–º
        from datasets.medical_tensors import MedicalTensorDataset

        # –í–∞—à —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–¥ –∑–¥–µ—Å—å
        print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...")

    except ImportError as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        print("–≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–¥—É–ª—è —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    """