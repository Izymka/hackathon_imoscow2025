import gc
import json
import shutil
import tempfile
import zipfile
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Optional, Any, Dict
import logging
import asyncio
import subprocess
import torch
import time
from pathlib import Path
import urllib.request
import os
import fcntl
import numpy as np
import pandas as pd
import requests

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è ML –º–æ–¥–µ–ª–∏
import sys

sys.path.append('../chest_ct_ai_classifier/src')
sys.path.append('./chest_ct_ai_classifier/src')

from chest_ct_ai_classifier.src.model.prepare_tensor import prepare_ct_tensor
from chest_ct_ai_classifier.src.model.inference import MedicalModelInference
from chest_ct_ai_classifier.src.model.config import ModelConfig
from chest_ct_ai_classifier.src.utils.dicom_parser import parse_dicom, DicomSummary

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –º–æ–¥–µ–ª–∏
ml_model = None


def convert_numpy_types(obj: Any) -> Any:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç numpy –∏ –¥—Ä—É–≥–∏–µ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã –≤ –Ω–∞—Ç–∏–≤–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ JSON.
    """
    # –õ–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    try:
        from pydicom.multival import MultiValue  # type: ignore
    except Exception:  # –µ—Å–ª–∏ pydicom –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        MultiValue = tuple  # –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞, —á—Ç–æ–±—ã isinstance —Ä–∞–±–æ—Ç–∞–ª –±–µ–∑ pydicom

    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.cpu().numpy().tolist()
    elif isinstance(obj, (set, frozenset)):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (datetime,)):
        return obj.isoformat()
    elif isinstance(obj, Path):
        return str(obj)
    elif isinstance(obj, MultiValue):
        # pydicom MultiValue -> —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
        return [str(item) for item in obj]
    elif isinstance(obj, dict):
        return {str(key): convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫—É
        try:
            return str(obj)
        except Exception:
            return obj


# –ü—É—Ç—å –∫ —Å–∫—Ä–∏–ø—Ç—É –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
PREPARE_CT_SCRIPT = os.getenv("PREPARE_CT_SCRIPT", "chest_ct_ai_classifier/src/scripts/prepare_ct_medicalnet_format.py")
VALID_TYPES = os.getenv("VALID_TYPES", "CHEST,CHEST_TO_PELVIS").split(",")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    global ml_model
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
        weights_url = "https://ct-dicom-storage.website.yandexcloud.net/model/weights.pth"
        model_dir = Path("./model")
        model_path = model_dir / "weights.pth"

        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        model_dir.mkdir(parents=True, exist_ok=True)

        # –ú–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, —á—Ç–æ–±—ã –Ω–µ —Å–∫–∞—á–∏–≤–∞—Ç—å –≤–µ—Å–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–æ—Ä–∫–µ—Ä–∞—Ö
        lock_path = model_dir / "weights.lock"
        with open(lock_path, "w") as lock_file:
            logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            try:
                # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∑–∞—Ö–≤–∞—Ç–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if not model_path.exists():
                    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
                    tmp_path = model_path.with_suffix(".tmp")
                    try:
                        if tmp_path.exists():
                            tmp_path.unlink(missing_ok=True)
                        start_time = time.time()
                        urllib.request.urlretrieve(weights_url, str(tmp_path))
                        # –ê—Ç–æ–º–∞—Ä–Ω–æ –∑–∞–º–µ–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ —Ü–µ–ª–µ–≤–æ–π
                        os.replace(tmp_path, model_path)
                        logger.info(f"‚úÖ –í–µ—Å–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
                    except Exception as download_error:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤: {download_error}")
                        # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ —Å–ª—É—á–∞–π —á–∞—Å—Ç–∏—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                        try:
                            if tmp_path.exists():
                                tmp_path.unlink(missing_ok=True)
                        except Exception:
                            pass
                        raise
                else:
                    logger.info(f"‚úÖ –í–µ—Å–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {model_path}")
            finally:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)

        config = ModelConfig()
        ml_model = MedicalModelInference(str(model_path), config)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise

    yield

    # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    ml_model = None


app = FastAPI(
    title="DICOM Parser Service",
    version="0.1.0",
    lifespan=lifespan
)


class PredictionResponse(BaseModel):
    success: bool
    patient_id: str
    prediction: Dict[str, Any]
    processing_time: float
    metadata: Optional[Dict[str, Any]] = None
    message: Optional[str] = None


class ProcessRequest(BaseModel):
    zip_path: str
    extract_subdir: Optional[str] = None  # optional: if user knows exact inner dir


def _safe_is_zip(path: Path) -> bool:
    try:
        return zipfile.is_zipfile(path)
    except Exception:
        return False


def read_tensor(path: Path) -> torch.Tensor:
    """–ß—Ç–µ–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–∞ –∏–∑ PyTorch —Ñ–∞–π–ª–∞ (.pt, .pth)"""
    try:
        tensor = torch.load(path, weights_only=False, map_location='cpu')

        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å 'state_dict' –∏–ª–∏ 'tensor', –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–Ω–∑–æ—Ä
        if isinstance(tensor, dict):
            if 'tensor' in tensor:
                tensor = tensor['tensor']
            elif 'state_dict' in tensor:
                # –≠—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–Ω–∑–æ—Ä
                raise ValueError(
                    "–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç state_dict –º–æ–¥–µ–ª–∏, –∞ –Ω–µ —Ç–µ–Ω–∑–æ—Ä. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load_state_dict –¥–ª—è –º–æ–¥–µ–ª–∏.")
            else:
                # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–Ω–∑–æ—Ä, –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π
                for key, value in tensor.items():
                    if isinstance(value, torch.Tensor):
                        return value
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–Ω–∑–æ—Ä –≤ —Å–ª–æ–≤–∞—Ä–µ")

        if not isinstance(tensor, torch.Tensor):
            raise ValueError(f"–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–Ω–∑–æ—Ä, –∞ —Å–æ–¥–µ—Ä–∂–∏—Ç: {type(tensor)}")

        return tensor

    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PyTorch —Ñ–∞–π–ª–∞: {e}")


def process_predict(dicom_dir, tensor_output_dir, background_tasks: BackgroundTasks,
                    temp_dir: Path) -> PredictionResponse:
    global ml_model
    if ml_model is None:
        raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    start_time = time.time()

    try:
        tensor_result = prepare_ct_tensor(dicom_dir, tensor_output_dir, debug=True)
        logger.info("Tensors prepared", tensor_result)
    except Exception as e:
        raise HTTPException(
            status_code=509,
            detail=str(e)
        )

    # –ü–æ–∏—Å–∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
    tensor_files = list(tensor_output_dir.glob("*.pt"))
    if not tensor_files:
        raise HTTPException(
            status_code=500,
            detail="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –∏–∑ DICOM —Ñ–∞–π–ª–æ–≤"
        )

    tensor_file = tensor_files[0]

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    tensor = read_tensor(tensor_file)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    prediction = ml_model.predict(tensor)
    # explanation = ml_model.explain_prediction(tensor, method="saliency", visualize=False, target_class=prediction["prediction"])
    del tensor
    gc.collect()

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ numpy —Ç–∏–ø–æ–≤ –≤ Python —Ç–∏–ø—ã –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    prediction = convert_numpy_types(prediction)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –ø–∞—Ü–∏–µ–Ω—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    patient_id = tensor_file.stem

    processing_time = time.time() - start_time

    # –ü–ª–∞–Ω–∏—Ä—É–µ–º –æ—á–∏—Å—Ç–∫—É –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ñ–æ–Ω–µ
    background_tasks.add_task(cleanup_temp_dir, temp_dir)

    return PredictionResponse(
        success=True,
        patient_id=patient_id,
        prediction=prediction,
        processing_time=processing_time,
        message="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ"
    )


@app.get("/healthcheck")
def healthcheck() -> Dict[str, Any]:
    return {
        "status": "ok",
        "model_loaded": ml_model is not None,
        "service": "DICOM Normality Prediction",
        "time": datetime.now().isoformat()
    }


@app.post("/process")
def process(req: ProcessRequest, background_tasks: BackgroundTasks) -> Dict[str, Any]:
    zip_path = Path(req.zip_path)

    logger.info(f"Processing DICOM archive: {zip_path}")

    if not zip_path.exists():
        raise HTTPException(status_code=400, detail=f"Zip path does not exist: {zip_path}")
    if not zip_path.is_file():
        raise HTTPException(status_code=400, detail=f"Path is not a file: {zip_path}")
    if not _safe_is_zip(zip_path):
        raise HTTPException(status_code=400, detail=f"Path is not a valid zip archive: {zip_path}")

    tmpdir = Path(tempfile.mkdtemp(prefix="dicom_zip_"))
    extract_root = tmpdir / "extracted"
    extract_root.mkdir(parents=True, exist_ok=True)
    tensors_dir = tmpdir / "tensors"
    tensors_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Extracting zip archive to: {extract_root}")

    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            zf.extractall(extract_root)

        # If user specified a subdir, use it; else try to pick a sensible directory to parse
        dicom_root = extract_root
        if req.extract_subdir:
            candidate = extract_root / req.extract_subdir
            if candidate.exists() and candidate.is_dir():
                dicom_root = candidate
            else:
                raise HTTPException(status_code=510, detail=f"Provided extract_subdir not found: {candidate}")
        else:
            # If the archive exploded into a single top-level folder, descend into it
            children = [p for p in extract_root.iterdir() if p.is_dir()]
            if len(children) == 1:
                dicom_root = children[0]

        try:
            summary: Optional[DicomSummary] = parse_dicom(dicom_root, True)
            if summary is None:
                raise HTTPException(511, "No DICOM files found after extraction.")
            if summary.body_part_examined not in VALID_TYPES:
                raise HTTPException(512,
                                    f"Expected body part examined to be one of {VALID_TYPES}, but got {summary.body_part_examined}")
            if summary.study_uid is None:
                raise Exception("Study UID not found")

            predict = process_predict(dicom_root, tensors_dir, background_tasks, tmpdir)

            # Convert to plain dict for JSON response
            dicom_summary = summary.to_dict() if hasattr(summary, 'to_dict') else dict(summary)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ xlsx —Ñ–∞–π–ª
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä—è–¥–æ–º —Å –∞—Ä—Ö–∏–≤–æ–º
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                random_suffix = np.random.randint(1000, 9999)
                result_xlsx_path = zip_path.with_name(f"{timestamp}_{random_suffix}.xlsx")

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                series_uids = None
                try:
                    # summary.series_uids –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º/–∫–æ—Ä—Ç–µ–∂–µ–º
                    series_uids = getattr(summary, "series_uids", None)
                    if isinstance(series_uids, (list, tuple)):
                        series_uids = ",".join(map(str, series_uids))
                except Exception:
                    series_uids = None

                data_row = {
                    "path_to_study": str(dicom_root),
                    "study_uid": getattr(summary, "study_uid", None),
                    "series_uid": series_uids if series_uids is not None else getattr(summary, "series_uids", None),
                    "probability_of_pathology": float(predict.prediction["probabilities"][1]) if isinstance(
                        predict.prediction.get("probabilities"), (list, tuple)) and len(
                        predict.prediction["probabilities"]) > 1 else None,
                    "pathology": int(predict.prediction.get("prediction")) if predict.prediction.get(
                        "prediction") is not None else None,
                    "processing_status": "Success",
                    "time_of_processing": float(predict.processing_time),
                }

                # –°–æ–∑–¥–∞—ë–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ .xlsx
                df = pd.DataFrame([data_row])
                df.to_excel(result_xlsx_path, index=False)
                xlsx_result_path = str(result_xlsx_path)
            except Exception as save_err:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ XLSX: {save_err}")
                xlsx_result_path = None

            return {
                "ok": True,
                "extracted_to": str(extract_root),
                "dicom_root": str(dicom_root),
                "result_path": xlsx_result_path,
                "result": {
                    "message": predict.message,
                    "processing_time": predict.processing_time,
                    "prediction": {
                        "result": predict.prediction["prediction"],
                        "confidence": predict.prediction["confidence"],
                        "probabilities": list(predict.prediction["probabilities"]),
                    }
                },
                "dicom": dicom_summary,
            }

        except Exception as e:
            logger.error(f"Error processing dicom {dicom_root}: {e}")
            return {
                "ok": False,
                "message": str(e),
                "extracted_to": str(extract_root),
                "dicom_root": str(dicom_root),
            }

    except HTTPException:
        # Let FastAPI handle these
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Processing failed: {e}")
    finally:
        try:
            background_tasks.add_task(cleanup_temp_dir, tmpdir)
        except Exception:
            pass


def cleanup_temp_dir(temp_dir: Path):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        logger.info(f"Cleanup started: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)
        logger.info(f"Cleanup complete: {temp_dir}")
    except Exception as e:
        logger.warning(f"Error cleaning dir {temp_dir}: {e}")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8010,
        reload=True,
        log_level="debug"
    )
